{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Debiasing with DiCE\n",
    "\n",
    "This notebook demonstrates how to use the adversarial debiasing feature in DiCE to increase model fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import oviya_dice\n",
    "from oviya_dice import Data, Model, Dice\n",
    "from oviya_dice.constants import BackEndTypes\n",
    "\n",
    "from oviya_dice.utils.helpers import load_adult_income_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Adult Income Dataset\n",
    "\n",
    "We'll use the UCI Adult Income dataset, which predicts whether income exceeds $50K/yr based on census data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Single</td>\n",
       "      <td>White-Collar</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>Self-Employed</td>\n",
       "      <td>Assoc</td>\n",
       "      <td>Married</td>\n",
       "      <td>Professional</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>Private</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Married</td>\n",
       "      <td>White-Collar</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>Private</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Single</td>\n",
       "      <td>Service</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Self-Employed</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Married</td>\n",
       "      <td>White-Collar</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age      workclass     education marital_status    occupation   race  \\\n",
       "0   28        Private     Bachelors         Single  White-Collar  White   \n",
       "1   30  Self-Employed         Assoc        Married  Professional  White   \n",
       "2   32        Private  Some-college        Married  White-Collar  White   \n",
       "3   20        Private  Some-college         Single       Service  White   \n",
       "4   41  Self-Employed  Some-college        Married  White-Collar  White   \n",
       "\n",
       "   gender  hours_per_week  income  \n",
       "0  Female              60       0  \n",
       "1    Male              65       1  \n",
       "2    Male              50       0  \n",
       "3  Female              35       0  \n",
       "4    Male              50       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "dataset = load_adult_income_dataset()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define protected attribute(s)\n",
    "protected_attributes = ['gender']\n",
    "\n",
    "# Create a Data object\n",
    "d = Data(dataframe=dataset, \n",
    "         continuous_features=['age', 'hours_per_week'], \n",
    "         outcome_name='income',\n",
    "         protected_attributes=protected_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Train a Model with Adversarial Debiasing\n",
    "\n",
    "Now we'll create a model using the adversarial debiasing approach to mitigate bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ALL', 'AdversarialDebiasing', 'Pytorch', 'Sklearn', 'Tensorflow1', 'Tensorflow2', 'VALA', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y7/g00g5c0s1l9fvqkz8t8k2z740000gn/T/ipykernel_14023/2830132001.py:3: UserWarning: adversarial_debiasing backend not in supported backends sklearn,TF1,TF2,PYT\n",
      "  model = Model(backend=BackEndTypes.AdversarialDebiasing,\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "should provide either a trained model or the path to a model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create model with adversarial debiasing\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mdir\u001b[39m(BackEndTypes))\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBackEndTypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdversarialDebiasing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m              \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclassifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m              \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mohe-min-max\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m              \u001b[49m\u001b[43mprotected_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprotected_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m              \u001b[49m\u001b[43mdebias_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/594bb final proj/DiCE/oviya_dice/model.py:50\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, model, model_path, backend, model_type, func, kw_args, protected_attributes, debias_weight)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebias_weight \u001b[38;5;241m=\u001b[39m debias_weight\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m model_path \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould provide either a trained model or the path to a model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecide_implementation_type(model, model_path, backend, func, kw_args)\n",
      "\u001b[0;31mValueError\u001b[0m: should provide either a trained model or the path to a model"
     ]
    }
   ],
   "source": [
    "# Create model with adversarial debiasing\n",
    "print(dir(BackEndTypes))\n",
    "model = Model(backend=BackEndTypes.AdversarialDebiasing, \n",
    "              model_type=\"classifier\",\n",
    "              func=\"ohe-min-max\",\n",
    "              protected_attributes=protected_attributes,\n",
    "              debias_weight=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features, target, and protected attributes\n",
    "X = dataset.drop(['income', 'gender'], axis=1)\n",
    "y = dataset['income']\n",
    "protected = dataset['gender'].map({'Male': 1, 'Female': 0})\n",
    "\n",
    "# Train the model\n",
    "model.train_model(X, y, protected, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Fairness Metrics\n",
    "\n",
    "Let's evaluate the fairness of our model using various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate fairness\n",
    "fairness_metrics = model.evaluate_fairness(X, y, protected)\n",
    "print(\"Fairness Metrics:\")\n",
    "for metric, value in fairness_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Counterfactual Explanations\n",
    "\n",
    "Now we'll use DiCE to generate counterfactual explanations for our debiased model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use DiCE to generate counterfactual explanations\n",
    "exp = Dice(d, model, method=\"random\")\n",
    "query_instance = X.iloc[0:1]\n",
    "counterfactuals = exp.generate_counterfactuals(query_instance, total_CFs=5, desired_class=\"opposite\")\n",
    "\n",
    "# Display counterfactuals\n",
    "counterfactuals.visualize_as_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with a Standard Model\n",
    "\n",
    "Let's compare our debiased model with a standard model without debiasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a standard model without debiasing\n",
    "standard_model = Model(backend=\"sklearn\", model_type=\"classifier\", func=\"ohe-min-max\")\n",
    "\n",
    "# Train the standard model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Prepare the model\n",
    "numerical = ['age', 'hours_per_week']\n",
    "categorical = X.columns.difference(numerical)\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "transformations = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical)])\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', transformations),\n",
    "                      ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Train the model\n",
    "standard_model.model = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DiCE explainer for the standard model\n",
    "standard_exp = Dice(d, standard_model, method=\"random\")\n",
    "\n",
    "# Generate counterfactuals\n",
    "standard_counterfactuals = standard_exp.generate_counterfactuals(query_instance, total_CFs=5, desired_class=\"opposite\")\n",
    "\n",
    "# Display counterfactuals\n",
    "standard_counterfactuals.visualize_as_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Fairness of Standard Model\n",
    "\n",
    "Let's evaluate the fairness of our standard model for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import fairness metrics\n",
    "from dice_ml.utils.fairness_metrics import (\n",
    "    demographic_parity_difference,\n",
    "    equal_opportunity_difference,\n",
    "    disparate_impact_ratio,\n",
    "    equalized_odds_difference\n",
    ")\n",
    "\n",
    "# Get predictions from standard model\n",
    "standard_preds = standard_model.model.predict_proba(X)[:, 1]\n",
    "\n",
    "# Calculate fairness metrics\n",
    "standard_metrics = {\n",
    "    'demographic_parity_difference': demographic_parity_difference(standard_preds, protected),\n",
    "    'equal_opportunity_difference': equal_opportunity_difference(standard_preds, y, protected),\n",
    "    'disparate_impact_ratio': disparate_impact_ratio(standard_preds, protected),\n",
    "    'equalized_odds_difference': equalized_odds_difference(standard_preds, y, protected)\n",
    "}\n",
    "\n",
    "print(\"Standard Model Fairness Metrics:\")\n",
    "for metric, value in standard_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nAdversarial Debiased Model Fairness Metrics:\")\n",
    "for metric, value in fairness_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we demonstrated how to use adversarial debiasing in DiCE to create fairer models. We compared the fairness metrics between a standard model and an adversarially debiased model, showing how the debiasing approach can help reduce bias in machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
